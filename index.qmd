---
title: "Bayesian Networks - Data Science Capstone"
author: "Michael Zeihen, Maxime Martin, Meghan Alexander, Sarah Burns, Seth Henry"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

__NEW>>__ [Dataset & Summaries](Reference_Summaries.html)<br>
[Slides](Slides.html)<br>


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(rmarkdown)
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(datasets)
library(ggplot2)
library(cluster)
```


## Introduction

### What is "Bayesian Network"?

A Bayesian network, also known as a belief network or a probabilistic graphical model, is a representation of a set of variables and their conditional dependencies via a Directed Acyclic Graph (DAG). [@pearl1986fusion] Each node in the graph represents a random variable, and the edges between the nodes represent probabilistic dependencies among these variables. Bayes' theorem is a fundamental concept used in this network. Bayes' thereum describes the probability of an event, based on prior knowledge of conditions that might be related to the event. The formula for Bayes' theorem is:
$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

A simple Bayesian network can be visualized as a directed acyclic graph (DAG) with nodes representing random variables and directed edges representing conditional dependencies between these variables.[@koller2009probabilistic] 

<figure style="text-align: center;">
  <img src="img/Simple_Bayesian_Student.png" alt="Alt text" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption>Simple Bayesian Network</figcaption>
</figure>

Bayesian networks leverage Bayes' theorem to update the probabilities of various hypotheses given new evidence, allowing for probabilistic reasoning and decision-making under uncertainty. The process involves defining the network structure and conditional probabilities, calculating joint and marginal probabilities, and applying Bayes' theorem to infer the posterior probabilities.
[@koller2009probabilistic] 

### What are the advantages to using Bayesian Networks 

Data analysis offers a variety of tools like rule-based systems, decision trees, and artificial neural networks, along with techniques such as density estimation, classification, regression, and clustering.[@heckerman1995tutorial] In this landscape, what unique advantages do Bayesian networks and Bayesian methods bring to the table?

Handling Incomplete Data:

Bayesian networks can manage incomplete data sets effectively. They encode dependencies between variables, such as strong anti-correlations, which traditional supervised learning models struggle with when data for some variables is missing. This allows for accurate predictions even with incomplete information.

Learning Causal Relationships:

Bayesian networks facilitate learning causal relationships, which is crucial for understanding complex systems and making predictions under interventions. For example, determining if increasing advertisement exposure causes higher product sales can be analyzed using Bayesian networks, even without direct experimental data.

Combining Domain Knowledge and Data:

Bayesian networks integrate domain knowledge (prior knowledge) with data seamlessly. This is particularly beneficial when data is limited or costly. Bayesian networks have a causal semantics that makes it straightforward to incorporate prior knowledge about causal relationships, enhancing the accuracy and reliability of predictions.

Avoiding Overfitting:

Bayesian networks provide an efficient approach to prevent overfitting. They achieve this by "smoothing" models using all available data during training, eliminating the need to reserve data for testing purposes. This ensures models generalize well to new data and avoid capturing noise or spurious patterns from the training data.

## Dataset
We are looking for a vetted data set for our project. The Iris dataset is a well-known multivariate dataset in the field of machine learning and statistics. It was introduced by the British biologist and statistician Ronald Fisher in 1936 in his paper "The use of multiple measurements in taxonomic problems." The dataset contains 150 samples from three species of Iris flowers: Iris setosa, Iris versicolor, and Iris virginica. There are Four features that were measured from each sample: the length and the width of the sepals and petals, in centimeters[@wikipedia_iris]

Variables:

- Sepal length (in cm)
- Sepal width (in cm)
- Petal length (in cm)
- Petal width (in cm)

Classes:

- Iris setosa
- Iris versicolor
- Iris virginica

Since we are looking at contious variables we believe it would help to convert the values to discrete values. Using discrete variables will simplify the structure of the Bayesian network and  reduces the complexity of the conditional probability tables (CPTs).  

** Option for team- we just we need to decide if ranges are based on what we believe the ranges are from our experience or we could go a step further and do clustering like kmeans to automatically create the groupings. This will be a good spot for visual of k-cluster of the different class features**

** just need to work cluster labels better

```{r, echo=F}
iris_data <- iris[, -5]

# Scale the data
iris_scaled <- scale(iris_data)

# Set a random seed for reproducibility
set.seed(123)

# Apply k-means clustering with 3 clusters
kmeans_result <- kmeans(iris_scaled, centers = 3, nstart = 20)

# Add cluster assignment to the original data
iris$Sepal.Length.Cluster <- as.factor(kmeans(iris_scaled[,1], centers = 3, nstart = 20)$cluster)
iris$Sepal.Width.Cluster <- as.factor(kmeans(iris_scaled[,2], centers = 3, nstart = 20)$cluster)
iris$Petal.Length.Cluster <- as.factor(kmeans(iris_scaled[,3], centers = 3, nstart = 20)$cluster)
iris$Petal.Width.Cluster <- as.factor(kmeans(iris_scaled[,4], centers = 3, nstart = 20)$cluster)

# Map cluster numbers to categories
cluster_map <- c("Small", "Medium", "Large")
iris$Sepal.Length.Category <- cluster_map[as.numeric(iris$Sepal.Length.Cluster)]
iris$Sepal.Width.Category <- cluster_map[as.numeric(iris$Sepal.Width.Cluster)]
iris$Petal.Length.Category <- cluster_map[as.numeric(iris$Petal.Length.Cluster)]
iris$Petal.Width.Category <- cluster_map[as.numeric(iris$Petal.Width.Cluster)]

# Print the first few rows to see the categories
# print(head(iris))
```

```{r}
# Remove the species column to cluster only based on measurements


# Visualize the clusters for Sepal Length and Sepal Width
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Sepal.Length.Category, shape = Sepal.Width.Category)) +
  geom_point(size = 3) +
  labs(title = "Clustering of Sepal Length and Width",
       x = "Sepal Length",
       y = "Sepal Width") +
  theme_minimal()

# Visualize the clusters for Petal Length and Petal Width
ggplot(iris, aes(Petal.Length, Petal.Width, color = Petal.Length.Category, shape = Petal.Width.Category)) +
  geom_point(size = 3) +
  labs(title = "Clustering of Petal Length and Width",
       x = "Petal Length",
       y = "Petal Width") +
  theme_minimal()
```

## Continous to Discrete Values ("Value range TBD")

| Label | Level  | Sepal Length | Sepal Width | Petal Length | Petal Width |
|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Small   | 1  | 4.0-5.5 | 2.0-2.75 |1-2 |0.0-.75 |
| Medium  | 2  | 5.6-6.5 | 2.76-3.25|3-5 |1.0-1.75 |
| Large   | 3  | 6.5-8.0 | 3.26-4.5 |5-7 |1.76-2.5 |

## Methods

Classification is a fundamental task in data analysis and pattern recognition where the goal is to build a classifierâ€”a function that assigns class labels to instances based on their attributes. Inducing classifiers from datasets of preclassified instances is a key challenge in machine learning. Many methods address this challenge using diverse functional representations like decision trees, decision lists, neural networks, decision graphs, and rules. One of the most effective classifiers, in the sense that its predictive performance is competitive
with state-of-the-art classifiers, is the so-called naive Bayesian classifier.[@friedman1997bayesian]

Define Variables and Dependencies:

Assign Conditional Probability Tables (CPTs): 

Inference:

Learning: structure and parameter 

## Analysis and Results
Decision Support:

Validation and Sensitivity Analysis:
monte carlo 

## Data and Visualization
directed acyclic graph (DAG).

Conditional Probability Tables (CPTs)

Probability Distributions:

## Statistical Modeling
Add discription of final model possibly after learning

### Conclusion
Add conclusion here

## References
