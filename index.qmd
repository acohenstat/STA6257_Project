---
title: "Predicting Income with Bayesian Networks: A Case Study Using the Adult Income Dataset - Data Science Capstone"
author: "Michael Zeihen, Maxime Martin, Sarah Burns, Seth Henry"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
toc: true
toc-title: "Table of Contents"
toc-depth: 3 
---

**NEW\>\>** [Dataset & Summaries](Reference_Summaries.html)<br>
[Slides](Slides.html)<br>

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(rmarkdown)
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(datasets)
library(ggplot2)
library(cluster)
library(bnlearn)
library(e1071)
library(Rgraphviz)
library(tidyr)
library(dplyr)
```

## Introduction

### What is "Bayesian Network"?

A Bayesian network, a belief network, or a probabilistic graphical model
represents a set of variables and their conditional dependencies via a
Directed Acyclic Graph (DAG). [@pearl1986fusion] Each node in the graph
represents a random variable, and the edges between the nodes represent
probabilistic dependencies among these variables. Bayes' theorem is a
fundamental concept used in this network. Bayes' thereum describes the
probability of an event, based on prior knowledge of conditions that
might be related to the event. The formula for Bayes' theorem is $$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

### What is Bayesian Theorem

A simple Bayesian network can be visualized as a directed acyclic graph
(DAG) with nodes representing random variables and directed edges
representing conditional dependencies between these variables.
[@koller2009probabilistic]

<figure style="text-align: center;">

<img src="img/Simple_Bayesian_Student.png" alt="Alt text" style="display: block; margin: 0 auto; width: 50%;"/>

<figcaption>Figure 1: Simple Bayesian Network</figcaption>

</figure>

Bayesian networks leverage Bayes' theorem to update the probabilities of
various hypotheses given new evidence, allowing for probabilistic
reasoning and decision-making under uncertainty. The process involves
defining the network structure and conditional probabilities,
calculating joint and marginal probabilities, and applying Bayes'
theorem to infer the posterior probabilities. [@koller2009probabilistic]

<p style="color: blue;">

Idea - Go over the bayes' theorem equation.

</p>

<p style="color: blue;">

Idea - Discuss Propagation.

</p>

### What are the advantages of using Bayesian Networks

Data analysis offers a variety of tools like rule-based systems,
decision trees, and artificial neural networks, along with techniques
such as density estimation, classification, regression, and
clustering.[@heckerman1995tutorial] What unique advantages do Bayesian
networks and Bayesian methods bring to the table in this landscape?

From the paper "A Bayesian Method for the Induction of Probabilistic Networks from Data"[@cooper1992bayesian], here are some of the advantages mentions using Bayesian Networks:

Handling Incomplete Data:

Bayesian networks facilitate learning causal relationships, which are
crucial for understanding complex systems and making predictions under
interventions. For example, determining if increasing advertisement
exposure causes higher product sales can be analyzed using Bayesian
networks, even without direct experimental data.

Learning Causal Relationships:

Bayesian networks facilitate learning causal relationships, which is
crucial for understanding complex systems and making predictions under
interventions. For example, determining if increasing advertisement
exposure causes higher product sales can be analyzed using Bayesian
networks, even without direct experimental data.

Combining Domain Knowledge and Data:

Bayesian networks seamlessly integrate domain knowledge (prior
knowledge) with data. This is particularly beneficial when data is
limited or costly. Their causal semantics make it straightforward to
incorporate prior knowledge about causal relationships, enhancing the
accuracy and reliability of predictions.

Avoiding Overfitting:

Bayesian networks provide an efficient approach to prevent overfitting.
They achieve this by "smoothing" models using all available data during
training, eliminating the need to reserve data for testing purposes.
This ensures that models generalize well to new data and avoid capturing
noise or spurious patterns from the training data.

### Applications of Bayesian Networks

Bayesian networks are widely applied across many diverse fields due to
their capabilities in modeling probabilistic relationships. A few
examples being - In healthcare, they can be employed to examine how
psychiatric, demographic, and socio-economic variables interrelate
[@bilek2018investigation]; in supply chain management, they can play a
crucial role in assessing risks, evaluating resilience, and analyzing
ripple effects - aiding in making informed decisions among uncertainties
[@hosseini2020bayesian]; and in political studies, they can assist to
predict online participation patterns by analyzing user interactions and
engagement metrics [@kopacheva2021predicting]. Bayesian networks provide
dynamic modeling, learning, and inference - enabling an understanding of
evolving systems over time in various applications
[@shiguihara2021dynamic]. All these example applications underscore the
utility of Bayesian networks in addressing complex probabilistic
challenges across different domains, and further their practical
usefulness.

### Components of Bayesian Networks

Bayesian networks are sophisticated models that represent the
probabilistic relationships among a set of variables using a directed
acyclic graph (DAG). The primary components of Bayesian networks include
nodes, edges, and conditional probability distributions, each playing a
crucial role in defining the structure and behavior of the network.

Nodes in a Bayesian network represent random variables. These variables
can be classified into several categories based on their roles within
the network. Parent nodes are those that have outgoing edges to other
nodes, signifying causal or influential relationships. Child nodes, on
the other hand, are the recipients of these edges, representing
variables that are influenced or caused by the parent nodes.
Additionally, evidence nodes are those for which we have observed data,
and these observations can be used to update the beliefs about other
variables in the network. Query nodes are the variables of interest, for
which we seek to compute posterior probabilities given the evidence.
This categorization helps in organizing and understanding the flow of
information within the network [@pearl1986fusion].

Edges in a Bayesian network are directed and represent probabilistic
dependencies between the nodes. An edge from node A to node B indicates
that A is a parent of B, and the probability distribution of B is
conditionally dependent on the state of A. This directional relationship
is a critical aspect of Bayesian networks, as it encodes the causal
assumptions and the flow of influence among the variables
[@pearl1986fusion].

The structure of a Bayesian network is a directed acyclic graph (DAG). A
DAG is a graph that is directed and contains no cycles, meaning it is
impossible to start at any node and follow a consistent direction along
the edges to return to the starting node. This acyclic property is
fundamental because it ensures that there is no infinite loop in the
probabilistic dependencies, allowing for coherent probabilistic
inferences. The DAG structure facilitates the decomposition of the joint
probability distribution of the variables into a product of conditional
probability distributions, greatly simplifying the representation and
computation of complex probabilistic models [@koller2009probabilistic].

Probabilistic inferences in Bayesian networks involve computing the
posterior probabilities of query nodes given the evidence nodes. This
process uses the network’s structure and the conditional probability
distributions to propagate information and update beliefs. One of the
key concepts in performing these inferences is conditional independence,
which allows for significant computational efficiency. Conditional
independence means that a variable is independent of another variable
given a set of other variables, reducing the number of direct
dependencies that need to be considered [@koller2009probabilistic].

Lastly, the conditional probability tree is a way to represent the
conditional probability distributions associated with each node in the
network. These trees show how the probability of each node depends on
the states of its parent nodes, providing a visual and computational
tool for working with the network’s probabilistic relationships
[@pearl1986fusion].

Understanding these components, nodes, edges, the directed acyclic
graph, probabilistic inferences, conditional independence and the
conditional probability tree provides a foundational framework for
working with Bayesian networks, enabling the modeling of complex systems
with interdependent variables in a structured and efficient manner.

### Limitations of Bayesian Networks

Although Bayesian networks are valuable in modeling and predicting
probabilistic relationships, they have their limitations that can hinder
effectiveness.

**Complexity**: One issue is their complexity which increases with the
number of variables introduced, making them computationally intensive to
build and then maintain for large, highly connected networks. This makes
real-time inference and updates challenging. Data Requirements: Bayesian
networks are largely data-driven; thus, the data requirements make
sparse data sets potentially compromise reliability.

**Assumptions**: Bayesian networks mostly involve assumptions in the
definition of relationships between variables and often make use of
expert inputs, thus open to subjectivity and biases [@kubsch2021beyond].
Bayesian models assume that variables are conditionally independent
given their parents in the network. In this case, violations to these
assumptions affect the correctness of the predictions made with these
models.

**Structure:** The structure is another critical limitation. After a
Bayesian network structure has been developed, it does not automatically
adapt to new data or changes in the problem domain.

**Scalability**: Scalability is a significant limitation: large networks
need huge computational effort for exact inference and are mostly
approximated, hence less accurate [@jewell2009bayesian]. Bayesian
networks are also limited in that they cannot model complex
relationships and nonlinearity between variables; it makes them less
successful in modeling real-world complex interactions compared to other
approaches, say, neural networks. These limitations stress that careful
application and possible use of hybrid methodologies may be necessary
when solving complex problems with Bayesian networks.

## Methods

(need to expand on this section since we changed data set and goal)

dataset link:
<https://www.kaggle.com/datasets/uciml/adult-census-income>

The main goal is to predict whether an individual's income exceeds
\$50,000 per year based on various demographic and employment-related
attributes.

No longer doing classification of iris, we are deteremine based of
probabaility if a person will make more or less then 50k based on these
features:

### Steps to create a Bayesian netowrk
<figure style="text-align: center;">
<img src="img/Screenshot 2024-07-11_at_8.53.32 PM.png" alt="Alt text" style="display: block; margin: 0 auto; width: 50%;"/>
<figcaption>Figure 2: Steps necessary to build and use BNs for safety and
reliability analysis [@Sigurdsson2001]</figcaption>

</figure>

As described in “Bayesian belief nets for managing expert judgment and modeling reliability” [@Sigurdsson2001], here are the discriptions for each step.

### 1. Identify Variables

The first step in constructing a Bayesian Network is to identify all
relevant variables that will be included in the model. These variables
represent the different aspects of the problem domain that need to be
analyzed. Variables can be continuous or discrete and should be chosen
based on their relevance to the problem and the data available. With
continuous variables, the work will be more complicated when comes the
discretization because we should base our analyse on expert knowledge.
In the other hand, discrete variables are easier to handle but provides
less various informations about the data.

### 2. Identify Network Structure

Once the variables are identified, the next step is to determine the
structure of the network, which involves specifying the relationships
between these variables. This structure can be defined manually based on
expert knowledge or learned from data using algorithms. This part is
actually the base of the modelisation, any decision made here will
change everything on the final result. [@scutari2009learning]

The `bnlearn` package in R provides various algorithms for learning the
network structure from data, such as the Hill-Climbing algorithm, Tabu
search, and constraint-based methods like PC algorithm. Using the
 `graphviz.plot()`  function from the  `Rgraphviz`  package is also a
good support for visualization.

### 3. Specify Conditional Probabilities

After the network structure is established, the next step is to specify
the conditional probability distributions for each variable. For each
node in the network, the conditional probability distribution given its
parent nodes needs to be defined. Then we can get the conditional
probability tables (CPT) which are the results interface of our
modelisation.

These probabilities can be estimated from data or provided by domain
experts. The `bnlearn` package can be used to fit these probabilities to
data using maximum likelihood estimation or Bayesian estimation methods.
[@heckerman1995tutorial]

### 4. Enter Evidence

Entering evidence involves incorporating observed values for certain
variables into the Bayesian Network, a process often called
"instantiating" or "clamping" variables. This updates the network to
reflect the current state of knowledge, enabling the computation of
posterior probabilities for other variables. The purposes of entering
evidence include conditioning the network to calculate posterior
probabilities of unobserved variables, aiding in medical diagnosis by
using symptoms or test results to identify the likelihood of diseases,
and supporting decision-making by updating beliefs based on the most
probable outcomes. The process involves identifying which variables have
observed values from real-world data, experimental results, or expert
knowledge, and then updating the network with these values to adjust the
probabilities accordingly.

In `bnlearn`, evidence can be entered using the `cpquery` function,
which allows for conditioning on the evidence and querying the network
for the posterior probabilities of the remaining variables.

### 5. Propagate

Propagation in a Bayesian Network refers to the process of computing the
posterior distributions of the unobserved variables given the observed
evidence. This can be done using various exact and approximate inference
algorithms. [@pearl1986fusion]

The `bnlearn` package provides functions like `grain` for exact
inference and `bn.fit` for approximate inference using sampling methods
such as Markov Chain Monte Carlo (MCMC).

### 6. Interpret Result

The final step is to interpret the results obtained from the propagation
step. This involves analyzing the posterior distributions to make
decisions, predictions, or to gain insights into the relationships
between variables. \[@cowell\]

The results can be visualized using various plotting functions provided
by `bnlearn`, such as `graphviz.plot` to visualize the network structure
and the conditional probability tables. Here, since the graphviz.chart
is not displaying correctly all the resullts we have, we will use the
CPT as interface.


					


| Feature        |                             Description |
|:---------------|----------------------------------------:|
| age            |                                     age |
| workclass      |                               Work type |
| fnlwgt         | Current Population Survey (CPS) Weights |
| education      |                         Education level |
| education.num  |                  Education level number |
| marital.status |                          Marital Status |
| occupation     |                              occupation |
| relationship   |                     family relationship |
| race           |                                         |
| sex            |                                  Gender |
| capital.gain   |                            Capital Gain |
| capital.loss   |                            Capital Loss |
| hours.per.week |                  Working hours per week |
| native.country |                      Native nationality |
| income         |                                  income |

```{r, echo=T}
dataset <- read.csv("data/adult.csv")
dataset[dataset == "?"] <- NA
# TCP process becomes to large for limits of R(12.5gb) need to prunce some nodes atm

dataset <- na.omit(dataset)
dataset <- subset(dataset, select = -fnlwgt)
dataset <- subset(dataset, select = -education.num)


#dataset$fnlwgt <- as.factor(dataset$fnlwgt)

#dataset$education.num <- as.factor(dataset$education.num)


breaks <- c(18, 40, 65, 100)  # Adjust as needed
labels <- c("Young", "Middle", "Retired")
dataset$age <- cut(dataset$age, breaks = breaks, labels = labels, include.lowest = TRUE)

breaks <- c(0, 5000, 10000, 20000, 100000)  # Adjust as needed
labels <- c("Low", "Medium", "High", "Rich")
dataset$capital.gain  <- cut(dataset$capital.gain, breaks = breaks, labels = labels, include.lowest = TRUE)
dataset$capital.loss  <- cut(dataset$capital.loss, breaks = breaks, labels = labels, include.lowest = TRUE)

breaks <- c(0, 35, 40, 200)  # Adjust as needed
labels <- c("Partime", "FullTIme", "Overtime")
dataset$hours.per.week <- cut(dataset$hours.per.week, breaks = breaks, labels = labels, include.lowest = TRUE)

dataset$race <- as.factor(dataset$race)
dataset$native.country <- as.factor(dataset$native.country)
dataset$workclass <- as.factor(dataset$workclass)
dataset$education <- as.factor(dataset$education)
dataset$marital.status <- as.factor(dataset$marital.status)
dataset$occupation <- as.factor(dataset$occupation)
dataset$relationship <- as.factor(dataset$relationship)
dataset$sex <- as.factor(dataset$workclass)
dataset$income <- as.factor(dataset$income)
```

```{r,echo=T}
### this is bayes classifier code
# Split the data into training and test sets
#set.seed(123)  # For reproducibility
#sample_index <- sample(1:nrow(dataset), 0.7 * nrow(dataset))  # 70% training data
#train_data <- dataset[sample_index, ]
#test_data <- dataset[-sample_index, ]
### Train the Naive Bayes model
#nb_model <- naiveBayes(income ~ ., data = train_data)
#
## Predict on the test data
#predictions <- predict(nb_model, test_data)
#
## Evaluate the model
#confusion_matrix <- table(predictions, test_data$income)
#print(confusion_matrix)
#
## Calculate accuracy
#accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
#print(paste("Accuracy:", accuracy))
```

```{r,echo=F}
# ### Bayesian Network Structure using bnlearn
#network_string <- "[age|income][workclass|income][education|income][education.num|income][marital.status|income][occupation|income][relationship|income][race|income][sex|income][capital.gain|income][capital.loss|income]#[hours.per.week|income][native.country|income][income]"
#
#bnlearn:::check.modelstring(network_string)
#dag <- model2network(network_string)
## Visualize the DAG using Rgraphviz
#
#graphviz.plot(dag, main = "Naive Bayes Network for income Dataset")


```

```{r,echo=T}
features = c("age","workclass","capital.gain","capital.loss",  
"hours.per.week", "education","marital.status","relationship",  
"occupation","sex", "native.country","race",          
"income")

dag = empty.graph(features)

#conditional
dag = set.arc(dag, from = "age", to = "education")
dag = set.arc(dag, from = "education", to = "occupation")
dag = set.arc(dag, from = "occupation", to = "workclass")
dag = set.arc(dag, from = "workclass", to = "hours.per.week")
dag = set.arc(dag, from = "marital.status", to = "relationship")

dag = set.arc(dag, from = "sex", to = "occupation")
dag = set.arc(dag, from = "sex", to = "education")

dag = set.arc(dag, from = "race", to = "occupation")
dag = set.arc(dag, from = "race", to = "education")
dag = set.arc(dag, from = "hours.per.week", to = "income")

dag = set.arc(dag, from = "native.country", to = "income")
dag = set.arc(dag, from = "relationship", to = "income")
dag = set.arc(dag, from = "capital.gain", to = "income")
dag = set.arc(dag, from = "capital.loss", to = "income")

graphviz.plot(dag, layout = "fdp")

```

```{r,echo=F}
# CPT Visualization using bnlearn
#print(features)
#print(names(dataset))
#print(nodes(dag))


fit <- bn.fit(dag, data = dataset, method = "bayes")

## Print CPTs
#for(node in nodes(fit)) {
#  cat("## CPT for", node, ":\n")
#  cpt <- fit[[node]]$prob
#  if (is.table(cpt)) {
#    print(knitr::kable(as.data.frame(cpt)))
#  } else {
#    for (state in names(cpt)) {
#      cat("### State:", state, "\n")
#      print(knitr::kable(as.data.frame(cpt[[state]])))
#    }
#  }
#  cat("\n")
#}
```
[CPTs for Income](cpt.html)

Define Variables and Dependencies:

Assign Conditional Probability Tables (CPTs):

Inference:

Learning: structure and parameter

<p style="color: blue;">

Idea - Define the problem.

</p>

<p style="color: blue;">

Idea - Define the structure.

</p>

<p style="color: blue;">

Idea - create the paramter learning CPT

</p>

<p style="color: blue;">

Idea - use belief propagation to comput probabilities

</p>

## Analysis and Results

### Dataset

This data was sourced from the 1994 Census Bureau database by Ronny
Kohavi and Barry Becker (Data Mining and Visualization, Silicon
Graphics). They extracted a set of relatively clean records using these
criteria: ((AAGE\>16) && (AGI\>100) && (AFNLWGT\>1) &&
(HRSWK\>0)).[@misc_adult_2] The objective is to predict whether an
individual earns more than \$50K annually.

```{r, echo=T}

head(dataset) %>%
  kable(caption = "Head of the Adult income Dataset")
```

Decision Support:

Validation and Sensitivity Analysis: monte carlo

<p style="color: blue;">

Idea - Evalulate the networks ability

</p>

<p style="color: blue;">

Idea - Cross Validate to test models robustness and generalization

</p>

<p style="color: blue;">

Idea - Asses how sensitive network is to input and structure chantes

</p>

<p style="color: blue;">

Idea -Use the networks to make a prediction

</p>

## Data and Visualization

```{r,echo=T}

ggplot(data = dataset, aes(x = age, fill = income)) +
  
# Add density plots for each income category
geom_bar(position = "stack", alpha = 0.8) +  # Adjust alpha for transparency

# Customize colors for each income category
scale_fill_manual(values = c("#FF5733", "#3349FF")) +  # Adjust colors as needed

# Label axes and add title
labs(x = "Age", y = "Count", title = "Age Distribution by Income") +

# Rotate x-axis labels for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

# Customize legend
guides(fill = guide_legend(title = "Income"))

```

```{r,echo=T}
ggplot(data = dataset, aes(x = workclass, fill = income)) +
  
# Add a histogram with stacked bars
geom_bar(position = "stack", alpha = 0.8) +  # Adjust alpha for transparency

# Customize colors for each income category
scale_fill_manual(values = c("#FF5733", "#3349FF")) +  # Adjust colors as needed

# Label axes and add title
labs(x = "Workclass", title = "Workclass") +

# Rotate x-axis labels for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

# Add legend for income categories
guides(fill = guide_legend(title = "Income"))
```

```{r,echo=T}
ggplot(data = dataset, aes(x = education, fill = income)) +
  
# Add a histogram with stacked bars
geom_bar(position = "stack", alpha = 0.8) +  # Adjust alpha for transparency

# Customize colors for each income category
scale_fill_manual(values = c("#FF5733", "#3349FF")) +  # Adjust colors as needed

# Label axes and add title
labs(x = "Education", title = "Education") +

# Rotate x-axis labels for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

# Add legend for income categories
guides(fill = guide_legend(title = "Income"))
```

directed acyclic graph (DAG).

Conditional Probability Tables (CPTs)

Probability Distributions:

## Statistical Modeling

Add discription of final model possibly after learning

### Conclusion

Add conclusion here

## References
