---
title: "Predicting Income with Bayesian Networks: A Case Study Using the Adult Income Dataset - Data Science Capstone"
author: "Michael Zeihen, Maxime Martin, Sarah Burns, Seth Henry"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
toc: true
toc-title: "Table of Contents"
toc-depth: 3 
---

**NEW\>\>** [Dataset & Summaries](Reference_Summaries.html)<br>
[Slides](Slides.html)<br>

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(rmarkdown)
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(datasets)
library(ggplot2)
library(cluster)
library(bnlearn)
library(e1071)
library(Rgraphviz)
library(tidyr)
library(dplyr)
```

## Introduction

In this study, we employ Bayesian Networks to predict whether individuals earn more than $50,000 annually based on demographic and employment-related attributes from the Adult Income Dataset. We use Bayesian Networks to model probabilistic dependencies among variables, providing a structured approach to understanding income factors. The goal is to develop a predictive model that identifies key factors influencing these income levels, offering a methodical approach to understanding socioeconomic dynamics.

### What is a "Bayesian Network"?

A Bayesian network, a belief network, or a probabilistic graphical model represents a set of variables and their conditional dependencies via a Directed Acyclic Graph (DAG). [@pearl1986fusion] EEach node in the graph represents a random variable, and the edges between the nodes represent probabilistic dependencies among these variables. Bayes' theorem is a fundamental concept used in this network. Bayes' theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event. The formula for Bayes' theorem is
$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

### What is the Bayesian Theorem

A simple Bayesian network can be visualized as a directed acyclic graph (DAG) with nodes representing random variables and directed edges representing conditional dependencies between these variables.
[@koller2009probabilistic]

<figure style="text-align: center;">

<img src="img/Simple_Bayesian_Student.png" alt="Alt text" style="display: block; margin: 0 auto; width: 50%;"/>

<figcaption>Figure 1: Simple Bayesian Network</figcaption>

</figure>

Bayesian networks leverage Bayes' theorem to update the probabilities of various hypotheses given new Evidence, allowing for probabilistic reasoning and decision-making under uncertainty. The process involves defining the network structure and conditional probabilities, calculating joint and marginal probabilities, and applying Bayes' theorem to infer the posterior probabilities. [@koller2009probabilistic]

<p style="color: blue;">
Idea - Go over the bayes' theorem equation.
</p>

<p style="color: blue;">
Idea - Discuss Propagation.
</p>

### What are the advantages of using Bayesian Networks

Data analysis offers a variety of tools like rule-based systems, decision trees, and artificial neural networks, along with techniques such as density estimation, classification, regression, and clustering. [@heckerman1995tutorial] What unique advantages do Bayesian networks and Bayesian methods bring to the table in this landscape?

In the paper "A Bayesian Method for the Induction of Probabilistic Networks from Data"[@cooper1992bayesian],  here are some of the advantages mentioned using Bayesian Networks:

Handling Incomplete Data:

Bayesian networks facilitate learning causal relationships, which are crucial for understanding complex systems and making predictions under interventions. For example, determining if increasing advertisement exposure causes higher product sales can be analyzed using Bayesian networks, even without direct experimental data.

Learning Causal Relationships:

Bayesian networks facilitate learning causal relationships, which are crucial for understanding complex systems and making predictions under interventions. For example, determining if increasing advertisement exposure causes higher product sales can be analyzed using Bayesian networks, even without direct experimental data.

Combining Domain Knowledge and Data:

Bayesian networks seamlessly integrate domain knowledge (prior knowledge) with data. This domain knowledge is particularly beneficial when data is limited or costly. Their causal semantics make it straightforward to incorporate prior knowledge about causal relationships, enhancing the accuracy and reliability of predictions.

Avoiding Overfitting:

Bayesian networks provide an efficient approach to prevent overfitting. They achieve this by "smoothing" models using all available data during training, eliminating the need to reserve data for testing purposes. Using all data ensures that models generalize well to new data and avoid capturing noise or spurious patterns from the training data.

### Applications of Bayesian Networks

We widely apply Bayesian networks across diverse fields due to their capabilities in modeling probabilistic relationships. A few examples are - In healthcare, they are used to examine how psychiatric, demographic, and socioeconomic variables interrelate [@bilek2018investigation]; in supply chain management, they can play a crucial role in assessing risks, evaluating resilience and analyzing ripple effects - aiding in making informed decisions among uncertainties[@hosseini2020bayesian]. In political studies, they can assist in predicting online participation patterns by analyzing user interactions and engagement metrics  [@kopacheva2021predicting]. Bayesian networks provide dynamic modeling, learning, and inference - enabling an understanding of evolving systems over time in various applications [@shiguihara2021dynamic]. All these example applications underscore the utility of Bayesian networks in addressing complex probabilistic challenges across different domains and further their practical usefulness.

### Components of Bayesian Networks

Bayesian networks are sophisticated models that represent probabilistic relationships among variables using a directed acyclic graph (DAG). The primary components of Bayesian networks include nodes, edges, and conditional probability distributions, each playing a crucial role in defining the network's structure and behavior.

Nodes in a Bayesian network represent random variables. The classification of these variables falls into several categories based on their roles within the network. Parent nodes have outgoing edges to other nodes, signifying causal or influential relationships. On the other hand, child nodes are the recipients of these edges, representing variables influenced or caused by the parent nodes. Additionally, evidence nodes are those for which we have observed data; using these observations can update the beliefs about other variables in the network. Query nodes are the variables of interest for which we seek to compute posterior probabilities given the Evidence. This categorization helps organize and understand the information flow within the network [@pearl1986fusion].

Edges in a Bayesian network are directed and represent probabilistic dependencies between the nodes. An edge from node A to node B indicates that A is a parent of B, and the probability distribution of B is conditionally dependent on the state of A. This directional relationship is a critical aspect of Bayesian networks, as it encodes the causal assumptions and the flow of influence among the variables es
[@pearl1986fusion].

The structure of a Bayesian network is a directed acyclic graph (DAG). A DAG is a directed graph containing no cycles. It is impossible to start at any node and follow a consistent direction along the edges to return to the starting node. This acyclic property is fundamental because it ensures no infinite loop in the probabilistic dependencies, allowing for coherent probabilistic inferences. The DAG structure facilitates the decomposition of the joint probability distribution of the variables into a product of conditional probability distributions, greatly simplifying the representation and computation of complex probabilistic models[@koller2009probabilistic].

Probabilistic inferences in Bayesian networks involve computing the posterior probabilities of query nodes given the evidence nodes. This process uses the network's structure and the conditional probability distributions to propagate information and update beliefs. One of the key concepts in performing these inferences is conditional independence, which allows for significant computational efficiency. Conditional independence means that a variable is independent of another variable given a set of other variables, reducing the number of direct dependencies that need to be considered [@koller2009probabilistic].

Lastly, the conditional probability tree represents the conditional probability distributions associated with each node in the network. These trees show how each node's probability depends on its parent nodes' states, providing a visual and computational tool for working with the network's probabilistic relationships [@pearl1986fusion].

Understanding these components, nodes, edges, the directed acyclic graph, probabilistic inferences, conditional independence, and the conditional probability tree provides a foundational framework for working with Bayesian networks, enabling the modeling of complex systems with interdependent variables in a structured and efficient manner.

### Limitations of Bayesian Networks

Although Bayesian networks are valuable in modeling and predicting
probabilistic relationships, they have their limitations that can hinder
effectiveness.

**Complexity**: One issue is their complexity, which increases with the number of variables introduced. Complexity makes Bayesian Networks computationally intensive to build and maintain for large, highly connected networks, making real-time inference and updates challenging. Data Requirements: Bayesian networks are largely data-driven; thus, the data requirements make sparse data sets potentially compromise reliability.

**Assumptions**: Bayesian networks mainly involve assumptions in defining relationships between variables and often use expert inputs, thus being open to subjectivity and biases [@kubsch2021beyond]. Bayesian models assume that variables are conditionally independent given their parents in the network. In this case, violations of these assumptions affect the correctness of the predictions made with these models.

**Structure:** The structure is another critical limitation. Developing a Bayesian network structure does not automatically adapt to new data or changes in the problem domain.

**Scalability**: Scalability is a significant limitation: Large networks require substantial computational effort for exact inference and typically rely on approximations, resulting in lower accuracy, noting this significant limitation [@jewell2009bayesian]. Bayesian networks are also limited in that they cannot model complex relationships and nonlinearity between variables; it makes them less successful in modeling real-world complex interactions compared to other approaches, say, neural networks. These limitations stress that careful application and possible use of hybrid methodologies may be necessary when solving complex problems with Bayesian networks.

## Methods

(need to expand on this section since we changed data set and goal)

dataset link:
<https://www.kaggle.com/datasets/uciml/adult-census-income>

The main goal is to predict whether an individual's income exceeds
\$50,000 per year based on various demographic and employment-related
attributes.



### Steps to create a Bayesian netowrk
<figure style="text-align: center;">
<img src="img/Screenshot 2024-07-11_at_8.53.32 PM.png" alt="Alt text" style="display: block; margin: 0 auto; width: 50%;"/>
<figcaption>Figure 2: Steps necessary to build and use BNs for safety and
reliability analysis [@Sigurdsson2001]</figcaption>

</figure>

Here are the descriptions for each step, as described in "Bayesian belief nets for managing expert judgment and modeling reliability" [@Sigurdsson2001].

__1. Identify Variables__

The first step in constructing a Bayesian Network is to identify and include all relevant variables for the model. These variables represent the different aspects of the problem domain that need to be analyzed. Variables can be continuous or discrete and should be chosen based on their relevance to the problem and available data. With continuous variables, the work will be more complicated regarding discretization because we should base our analysis on expert knowledge. On the other hand, discrete variables are easier to handle but provide less information about the data.

__2. Identify Network Structure__

The first step in constructing a Bayesian Network is to identify and include all relevant variables for the model. These variables represent the different aspects of the problem domain that need to be analyzed. Variables can be continuous or discrete and should be chosen based on their relevance to the problem and available data. With continuous variables, the work will be more complicated regarding discretization because we should base our analysis on expert knowledge. On the other hand, discrete variables are easier to handle but provide less information about the data. [@scutari2009learning]

The BNLearn package in R provides various algorithms for learning the network structure from data, such as the Hill-Climbing algorithm, Tabu search, and constraint-based methods like the PC algorithm. Using the graphviz.plot()  function from the  Rgraphviz  package, which also supports visualization.

__3. Specify Conditional Probabilities__

After establishing the network structure, the next step is specifying each variable's conditional probability distributions. Each node in the network must define its conditional probability distribution given its parent nodes. Then, we can get the conditional probability tables (CPT), the results interface of our modelization.

These probabilities can be estimated from data or provided by domain experts. The BNLearn package can fit these probabilities to data using maximum likelihood estimation or Bayesian estimation methods. 
[@heckerman1995tutorial]

__4. Enter Evidence__

Entering Evidence involves incorporating observed values for certain variables into the Bayesian Network, often called "instantiating" or "clamping" variables. This process updates the network to reflect the current state of knowledge, enabling the computation of posterior probabilities for other variables. The purposes of entering Evidence include conditioning the network to calculate posterior probabilities of unobserved variables, aiding in medical diagnosis by using symptoms or test results to identify the likelihood of diseases and supporting decision-making by updating beliefs based on the most probable outcomes. The process involves identifying which variables have observed values from real-world data, experimental results, or expert knowledge and then updating the network with these values to adjust the probabilities accordingly..

The cpquery function in BNLearn enables entering Evidence, conditioning on it, and querying the network to determine the posterior probabilities of the remaining variables.

__5. Propagate__

Propagation in a Bayesian Network refers to computing the posterior distributions of the unobserved variables given the observed Evidence. One can perform propagation using various exact and approximate inference algorithms.[@pearl1986fusion]

The BNLearn package provides functions like "grain" for exact inference and "bn.fit" for approximate inference using sampling methods such as Markov Chain Monte Carlo (MCMC).

__6. Interpret Result__

The final step is to interpret the results obtained from the propagation step. Interpreting results involves analyzing the posterior distributions to make decisions and predictions or to gain insights into the relationships between variables.  [@cowell]

Users can visualize the results using various plotting functions BNLearn offers, such as "graphviz.plot" to display the network structure and conditional probability tables. Due to issues with "graphviz.chart" not displaying all results correctly, we will utilize the CPT as an interface here.


					
### Define Variables and Dependencies:

| Feature        |                             Description |
|:---------------|----------------------------------------:|
| age            |                                     age |
| workclass      |                               Work type |
| fnlwgt         | Current Population Survey (CPS) Weights |
| education      |                         Education level |
| education.num  |                  Education level number |
| marital.status |                          Marital Status |
| occupation     |                              occupation |
| relationship   |                     family relationship |
| race           |                                         |
| sex            |                                  Gender |
| capital.gain   |                            Capital Gain |
| capital.loss   |                            Capital Loss |
| hours.per.week |                  Working hours per week |
| native.country |                      Native nationality |
| income         |                                  income |


__Age:__ Age is divided into three categories to account for the differences in education level and career progression. Younger individuals may still be in college and may not be as advanced in their career paths.

→ Categories: Young, Middle, Retired
→ Dependencies: Education

__Workclass:__ Type of employment affects income. There are vast variations in salary and benefits, as well as different career opportunities among different fields of study/work.
→ Categories: Federal-gov, Local-gov, Private, Self-employed-inc, Self-employed-not-inc, State-gov, Without-pay, etc.
→ Dependencies: Hours per Week

__Education:__ TThe level of education attained is a significant determinant because it typically measures eligibility for particular jobs, career promotion opportunities, and earnings. A higher education level means more income compared to lower education.
→ Categories: 10th - 12th grade, HS-grad, Some-college, Assoc-acdm, Assoc-voc, Bachelors, Masters, Prof-school, etc.
→ Dependencies: Occupation

__Marital Status:__ Marital status indicates the status of household dynamics and economic dependencies, which affect the level of income. For instance, married individuals may benefit from dual incomes, whereas divorced or widowed individuals may be responsible for other fiscal commitments and constraints.
→ Categories: Unmarried, Divorced, Separated, Never-married, Widowed, etc.
→ Dependencies: Relationship

__Occupation:__ The nature of the occupation directly determines income levels as a function of the varied pay scales and promotional opportunities among the different occupational categories.
→ Categories: Exec-managerial, Machine-op-inspct, Prof-speciality, Adm-clerical, Other-service, etc.
→ Dependencies: Workclass

__Relationship:__ The family relationship status might affect income because of financial dependencies or household dynamics such as a family with kids or no kids.
→ Categories: Not-in-family, Unmarried, Own-child, Other-relative, etc.
→ Dependencies: Income

__Race: Ethnicity.__
→ Categories: Black, White, Asian-Pac-Islander, etc.
→ Dependencies: Occupation, Education

__Capital Gain:__ Capital gain means additional sources of income and represents money invested financially, which influences total income.
→ Categories: Low, etc.
→ Dependencies: Income

__Capital Loss:__ Loss from Investments.
→ Categories: Low, etc.
→ Dependencies: Income

__Hours per Week:__ The hours put in work per week are directly associated with income, as it affects earnings coming through wages or salary. 
→ Categories: Partime, FullTime, Overtime, etc.
→ Dependencies: Income

__Native Country:__ Country of birth can affect income due to the effect of educational opportunities, conditions in the job market, and economic policy.
→ Category: United States, Mexico, Greece, etc.
→ Dependencies: Income

__Sex: Gender of Individual:__
→ Categories: Male, Female
→ Dependencies: Occupation, Education


### Data pre-processing:

Column Exclusions: We excluded the columns 'fnlwgt' and 'education.num' from the dataset. 'Education.num' duplicates the information already captured by the 'education' column, which represents education levels comprehensively. We removed 'Fnlwgt,' which contained Current Population Survey (CPS) weights because Bayesian Networks focus on modeling causal relationships among variables. CPS weights are statistical adjustments applied after sampling to correct biases rather than causal factors.

Age Grouping:  Age groups were defined to create bins for our Bayesian Network modeling. We categorized individuals aged 18 to 39 as 'Young,' 40 to 64 as 'Middle,' and individuals aged 65 to 100 as 'Retired.'

Capital Gain and Loss Grouping: We categorized capital gain and loss into distinct groups: 'Low' (0 to 4,999), 'Medium' (5,000 to 9,999), 'High' (10,000 to 19,999), and 'Rich' (20,000 to 100,000).

Hours per Week Grouping:  To facilitate analysis in our Bayesian network, hours worked per week were grouped into 'Partime' (0 to 34 hours), 'Fulltime' (35 to 39 hours), and 'Overtime' (40 to 200 hours) bins.

Factor Conversion: To facilitate modeling, we converted the following columns into categorical factors: 'race,' 'native.country', 'workclass,' 'education,' 'marital.status', 'occupation,' 'relationship,' 'sex,' and 'income.'


```{r, echo=T}
dataset <- read.csv("data/adult.csv")
dataset[dataset == "?"] <- NA
# TCP process becomes to large for limits of R(12.5gb) need to prunce some nodes atm

dataset <- na.omit(dataset)
dataset <- subset(dataset, select = -fnlwgt)
dataset <- subset(dataset, select = -education.num)


#dataset$fnlwgt <- as.factor(dataset$fnlwgt)

#dataset$education.num <- as.factor(dataset$education.num)


breaks <- c(18, 40, 65, 100)  # Adjust as needed
labels <- c("Young", "Middle", "Retired")
dataset$age <- cut(dataset$age, breaks = breaks, labels = labels, include.lowest = TRUE)

breaks <- c(0, 5000, 10000, 20000, 100000)  # Adjust as needed
labels <- c("Low", "Medium", "High", "Rich")
dataset$capital.gain  <- cut(dataset$capital.gain, breaks = breaks, labels = labels, include.lowest = TRUE)
dataset$capital.loss  <- cut(dataset$capital.loss, breaks = breaks, labels = labels, include.lowest = TRUE)

breaks <- c(0, 35, 40, 200)  # Adjust as needed
labels <- c("Partime", "FullTIme", "Overtime")
dataset$hours.per.week <- cut(dataset$hours.per.week, breaks = breaks, labels = labels, include.lowest = TRUE)

dataset$race <- as.factor(dataset$race)
dataset$native.country <- as.factor(dataset$native.country)
dataset$workclass <- as.factor(dataset$workclass)
dataset$education <- as.factor(dataset$education)
dataset$marital.status <- as.factor(dataset$marital.status)
dataset$occupation <- as.factor(dataset$occupation)
dataset$relationship <- as.factor(dataset$relationship)
dataset$sex <- as.factor(dataset$sex)
dataset$income <- as.factor(dataset$income)
```

```{r,echo=T}
### this is bayes classifier code
# Split the data into training and test sets
#set.seed(123)  # For reproducibility
#sample_index <- sample(1:nrow(dataset), 0.7 * nrow(dataset))  # 70% training data
#train_data <- dataset[sample_index, ]
#test_data <- dataset[-sample_index, ]
### Train the Naive Bayes model
#nb_model <- naiveBayes(income ~ ., data = train_data)
#
## Predict on the test data
#predictions <- predict(nb_model, test_data)
#
## Evaluate the model
#confusion_matrix <- table(predictions, test_data$income)
#print(confusion_matrix)
#
## Calculate accuracy
#accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
#print(paste("Accuracy:", accuracy))
```

```{r,echo=F}
# ### Bayesian Network Structure using bnlearn
#network_string <- "[age|income][workclass|income][education|income][education.num|income][marital.status|income][occupation|income][relationship|income][race|income][sex|income][capital.gain|income][capital.loss|income]#[hours.per.week|income][native.country|income][income]"
#
#bnlearn:::check.modelstring(network_string)
#dag <- model2network(network_string)
## Visualize the DAG using Rgraphviz
#
#graphviz.plot(dag, main = "Naive Bayes Network for income Dataset")


```

```{r,echo=T}
features = c("age","workclass","capital.gain","capital.loss",  
"hours.per.week", "education","marital.status","relationship",  
"occupation","sex", "native.country","race",          
"income")

dag = empty.graph(features)

#conditional
dag = set.arc(dag, from = "age", to = "education")
dag = set.arc(dag, from = "education", to = "occupation")
dag = set.arc(dag, from = "occupation", to = "workclass")
dag = set.arc(dag, from = "workclass", to = "hours.per.week")
dag = set.arc(dag, from = "marital.status", to = "relationship")

dag = set.arc(dag, from = "sex", to = "occupation")
dag = set.arc(dag, from = "sex", to = "education")

dag = set.arc(dag, from = "race", to = "occupation")
dag = set.arc(dag, from = "race", to = "education")
dag = set.arc(dag, from = "hours.per.week", to = "income")

dag = set.arc(dag, from = "native.country", to = "income")
dag = set.arc(dag, from = "relationship", to = "income")
dag = set.arc(dag, from = "capital.gain", to = "income")
dag = set.arc(dag, from = "capital.loss", to = "income")

graphviz.plot(dag, layout = "fdp")

```

```{r,echo=F}
# CPT Visualization using bnlearn
#print(features)
#print(names(dataset))
#print(nodes(dag))


fit <- bn.fit(dag, data = dataset, method = "bayes")

## Print CPTs
#for(node in nodes(fit)) {
#  cat("## CPT for", node, ":\n")
#  cpt <- fit[[node]]$prob
#  if (is.table(cpt)) {
#    print(knitr::kable(as.data.frame(cpt)))
#  } else {
#    for (state in names(cpt)) {
#      cat("### State:", state, "\n")
#      print(knitr::kable(as.data.frame(cpt[[state]])))
#    }
#  }
#  cat("\n")
#}
```
[CPTs for Income](cpt.html)

Define Variables and Dependencies:

Assign Conditional Probability Tables (CPTs):

Inference:

Learning: structure and parameter

<p style="color: blue;">
Idea - Define the problem.
</p>

<p style="color: blue;">
Idea - Define the structure.
</p>

<p style="color: blue;">
Idea - create the paramter learning CPT
</p>

<p style="color: blue;">
Idea - use belief propagation to comput probabilities
</p>

## Analysis and Results

### Dataset

This data was sourced from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker [@misc_adult_2]They extracted a set of relatively clean records using these criteria: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). (Becker and Kohavi 1996) The objective is to predict whether an individual earns more than $50K annually.

```{r, echo=T}

head(dataset) %>%
  kable(caption = "Head of the Adult income Dataset")
```

Decision Support:

Validation and Sensitivity Analysis: monte carlo

<p style="color: blue;">

Idea - Evalulate the networks ability

</p>

<p style="color: blue;">

Idea - Cross Validate to test models robustness and generalization

</p>

<p style="color: blue;">

Idea - Asses how sensitive network is to input and structure chantes

</p>

<p style="color: blue;">

Idea -Use the networks to make a prediction

</p>

## Data and Visualization

```{r,echo=T}

ggplot(data = dataset, aes(x = age, fill = income)) +
  
# Add density plots for each income category
geom_bar(position = "stack", alpha = 0.8) +  # Adjust alpha for transparency

# Customize colors for each income category
scale_fill_manual(values = c("#FF5733", "#3349FF")) +  # Adjust colors as needed

# Label axes and add title
labs(x = "Age", y = "Count", title = "Age Distribution by Income") +

# Rotate x-axis labels for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

# Customize legend
guides(fill = guide_legend(title = "Income"))

```

```{r,echo=T}
ggplot(data = dataset, aes(x = workclass, fill = income)) +
  
# Add a histogram with stacked bars
geom_bar(position = "stack", alpha = 0.8) +  # Adjust alpha for transparency

# Customize colors for each income category
scale_fill_manual(values = c("#FF5733", "#3349FF")) +  # Adjust colors as needed

# Label axes and add title
labs(x = "Workclass", title = "Workclass") +

# Rotate x-axis labels for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

# Add legend for income categories
guides(fill = guide_legend(title = "Income"))
```

```{r,echo=T}
ggplot(data = dataset, aes(x = education, fill = income)) +
  
# Add a histogram with stacked bars
geom_bar(position = "stack", alpha = 0.8) +  # Adjust alpha for transparency

# Customize colors for each income category
scale_fill_manual(values = c("#FF5733", "#3349FF")) +  # Adjust colors as needed

# Label axes and add title
labs(x = "Education", title = "Education") +

# Rotate x-axis labels for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

# Add legend for income categories
guides(fill = guide_legend(title = "Income"))
```

```{r,echo=T}

sex_income_count <- dataset %>%
  group_by(sex, income) %>%
  summarise(count = n()) %>%
  ungroup()

# Combine sex and income into a single factor with ordered levels
sex_income_count <- sex_income_count %>%
  mutate(sex_income = factor(interaction(sex, income),
                             levels = c("Male.<=50K", "Male.>50K", "Female.<=50K", "Female.>50K"),
                             labels = c("Male <=50K", "Male >50K", "Female <=50K", "Female >50K")))


# Create custom color palette
my_colors <- c("#0072B2", "#56B4E9", "#E69F00", "#F0E442")

# Create the pie chart with custom colors and clockwise rotation
ggplot(sex_income_count, aes(x = "", y = count, fill = sex_income)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 38) +  # Rotate the pie chart clockwise by 20%
  scale_fill_manual(values = my_colors) +  # Set custom colors
  labs(title = "Number of Males and Females Making Above and Below $50,000",
       fill = "Sex and Income") +
  theme_void() +
  theme(legend.position = "right")
```

```{r,echo=T}
occupation_gender_count <- dataset %>%
  group_by(occupation, sex) %>%
  summarise(count = n()) %>% 
  ungroup()

ggplot(occupation_gender_count, aes(x = occupation, y = count, fill = sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Number of Males and Females in Each Occupation",
       x = "Occupation",
       y = "Count",
       fill = "Sex")
```

```{r,echo=T}
occupation_income_count <- dataset %>%
  group_by(occupation, income) %>%
  summarise(count = n()) %>%
  ungroup()

ggplot(occupation_income_count, aes(x = occupation, y = count, fill = income)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Number of People in Each Occupation Making Over and Under $50,000",
       x = "Occupation",
       y = "Count",
       fill = "Income")
     
```





directed acyclic graph (DAG).

Conditional Probability Tables (CPTs)

Probability Distributions:

## Statistical Modeling

Add discription of final model possibly after learning

### Conclusion

Add conclusion here

## References
