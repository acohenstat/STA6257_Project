---
title: "Bayesian Networks - Data Science Capstone"
author: "Michael Zeihen, Maxime Martin, Meghan Alexander, Sarah Burns, Seth Henry"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
toc: true
toc-title: "Table of Contents"
toc-depth: 3 
---

__NEW>>__ [Dataset & Summaries](Reference_Summaries.html)<br>
[Slides](Slides.html)<br>


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(rmarkdown)
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(datasets)
library(ggplot2)
library(cluster)
library(bnlearn)
```


## Introduction

### What is "Bayesian Network"?

A Bayesian network, a belief network, or a probabilistic graphical model represents a set of variables and their conditional dependencies via a Directed Acyclic Graph (DAG). [@pearl1986fusion] Each node in the graph represents a random variable, and the edges between the nodes represent probabilistic dependencies among these variables. Bayes' theorem is a fundamental concept used in this network. Bayes' thereum describes the probability of an event, based on prior knowledge of conditions that might be related to the event. The formula for Bayes' theorem is
$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

### What is Bayesian Theorem


A simple Bayesian network can be visualized as a directed acyclic graph (DAG) with nodes representing random variables and directed edges representing conditional dependencies between these variables. [@koller2009probabilistic] 

<figure style="text-align: center;">
  <img src="img/Simple_Bayesian_Student.png" alt="Alt text" style="display: block; margin: 0 auto; width: 50%;">
  <figcaption>Simple Bayesian Network</figcaption>
</figure>

Bayesian networks leverage Bayes' theorem to update the probabilities of various hypotheses given new evidence, allowing for probabilistic reasoning and decision-making under uncertainty. The process involves defining the network structure and conditional probabilities, calculating joint and marginal probabilities, and applying Bayes' theorem to infer the posterior probabilities.
[@koller2009probabilistic] 

<p style="color: blue;">Idea - Go over the bayes' theorem equation.</p>

### What are the advantages of using Bayesian Networks

Data analysis offers a variety of tools like rule-based systems, decision trees, and artificial neural networks, along with techniques such as density estimation, classification, regression, and clustering.[@heckerman1995tutorial] What unique advantages do Bayesian networks and Bayesian methods bring to the table in this landscape?

Handling Incomplete Data:

Bayesian networks facilitate learning causal relationships, which are crucial for understanding complex systems and making predictions under interventions. For example, determining if increasing advertisement exposure causes higher product sales can be analyzed using Bayesian networks, even without direct experimental data.

Learning Causal Relationships:

Bayesian networks facilitate learning causal relationships, which is crucial for understanding complex systems and making predictions under interventions. For example, determining if increasing advertisement exposure causes higher product sales can be analyzed using Bayesian networks, even without direct experimental data.

Combining Domain Knowledge and Data:

Bayesian networks seamlessly integrate domain knowledge (prior knowledge) with data. This is particularly beneficial when data is limited or costly. Their causal semantics make it straightforward to incorporate prior knowledge about causal relationships, enhancing the accuracy and reliability of predictions.

Avoiding Overfitting:

Bayesian networks provide an efficient approach to prevent overfitting. They achieve this by "smoothing" models using all available data during training, eliminating the need to reserve data for testing purposes. This ensures that models generalize well to new data and avoid capturing noise or spurious patterns from the training data.

### Applications of Bayesian Networks
<p style="color: blue;">Idea - where are bayesian networks used.</p>

### Components of Bayesian Networks
<p style="color: blue;">Idea - talk about the components of bayesian networks( Nodes, Edges, DAG, CPT, Probablistic Inference).</p>

### Limitations of Bayesian Networks
<p style="color: blue;">Idea - talk about issues inherent with bayesian networks(Complexity, data, assumptions,structure, scalability).</p>

## Dataset
We are looking for a vetted data set for our project. The Iris dataset is a well-known multivariate dataset in machine learning and statistics. It was introduced by the British biologist and statistician Ronald Fisher in 1936 in his paper "The use of multiple measurements in taxonomic problems." The dataset contains 150 samples from three species of Iris flowers: Iris setosa, Iris versicolor, and Iris virginica. We measure four features from each sample: the length and width of the sepals and petals in centimeters.[@wikipedia_iris]

### Variables:
disctions

- Sepal length (in cm)
- Sepal width (in cm)
- Petal length (in cm)
- Petal width (in cm)

### Classes:

- Iris setosa
- Iris versicolor
- Iris virginica

Since we are looking at contious variables we believe it would help to convert the values to discrete values. Using discrete variables will simplify the structure of the Bayesian network and  reduces the complexity of the conditional probability tables (CPTs).  

** Option for team- we just we need to decide if ranges are based on what we believe the ranges are from our experience or we could go a step further and do clustering like kmeans to automatically create the groupings. This will be a good spot for visual of k-cluster of the different class features**

** just need to work cluster labels better

### Discrete values transformation in Iris Dataset

```{r, echo=F}
iris_data <- iris[, -5]

# Scale the data
iris_scaled <- scale(iris_data)

# Set a random seed for reproducibility
set.seed(123)

# Apply k-means clustering with 3 clusters
kmeans_result <- kmeans(iris_scaled, centers = 3, nstart = 20)

# Add cluster assignment to the original data
iris$Sepal.Length.Cluster <- as.factor(kmeans(iris_scaled[,1], centers = 3, nstart = 20)$cluster)
iris$Sepal.Width.Cluster <- as.factor(kmeans(iris_scaled[,2], centers = 3, nstart = 20)$cluster)
iris$Petal.Length.Cluster <- as.factor(kmeans(iris_scaled[,3], centers = 3, nstart = 20)$cluster)
iris$Petal.Width.Cluster <- as.factor(kmeans(iris_scaled[,4], centers = 3, nstart = 20)$cluster)

# Map cluster numbers to categories
cluster_map <- c("Small", "Medium", "Large")
iris$Sepal.Length.Category <- cluster_map[as.numeric(iris$Sepal.Length.Cluster)]
iris$Sepal.Width.Category <- cluster_map[as.numeric(iris$Sepal.Width.Cluster)]
iris$Petal.Length.Category <- cluster_map[as.numeric(iris$Petal.Length.Cluster)]
iris$Petal.Width.Category <- cluster_map[as.numeric(iris$Petal.Width.Cluster)]

# Print the first few rows to see the categories
# print(head(iris))
```

```{r}
# Remove the species column to cluster only based on measurements


# Visualize the clusters for Sepal Length and Sepal Width
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Sepal.Length.Category, shape = Sepal.Width.Category)) +
  geom_point(size = 3) +
  labs(title = "Clustering of Sepal Length and Width",
       x = "Sepal Length",
       y = "Sepal Width") +
  theme_minimal()

# Visualize the clusters for Petal Length and Petal Width
ggplot(iris, aes(Petal.Length, Petal.Width, color = Petal.Length.Category, shape = Petal.Width.Category)) +
  geom_point(size = 3) +
  labs(title = "Clustering of Petal Length and Width",
       x = "Petal Length",
       y = "Petal Width") +
  theme_minimal()
```

__Continous to Discrete Values ("Value range TBD")__

| Label | Level  | Sepal Length | Sepal Width | Petal Length | Petal Width |
|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Small   | 1  | 4.0-5.5 | 2.0-2.75 |1-2 |0.0-.75 |
| Medium  | 2  | 5.6-6.5 | 2.76-3.25|3-5 |1.0-1.75 |
| Large   | 3  | 6.5-8.0 | 3.26-4.5 |5-7 |1.76-2.5 |
```{r,echo=F}
# Define Bayesian network structure
network_structure <- "Species ~ Sepal.Length.Category + Sepal.Width.Category + Petal.Length.Category + Petal.Width.Category"

table_md <- knitr::kable(head(iris), format = "markdown")
```
| Sepal.Length| Sepal.Width| Petal.Length| Petal.Width|Species |Sepal.Length.Cluster |Sepal.Width.Cluster |Petal.Length.Cluster |Petal.Width.Cluster |Sepal.Length.Category |Sepal.Width.Category |Petal.Length.Category |Petal.Width.Category |
|------------:|-----------:|------------:|-----------:|:-------|:--------------------|:-------------------|:--------------------|:-------------------|:---------------------|:--------------------|:---------------------|:--------------------|
|          5.1|         3.5|          1.4|         0.2|setosa  |1                    |3                   |1                    |1                   |Small                 |Large                |Small                 |Small                |
|          4.9|         3.0|          1.4|         0.2|setosa  |1                    |1                   |1                    |1                   |Small                 |Small                |Small                 |Small                |
|          4.7|         3.2|          1.3|         0.2|setosa  |1                    |1                   |1                    |1                   |Small                 |Small                |Small                 |Small                |
|          4.6|         3.1|          1.5|         0.2|setosa  |1                    |1                   |1                    |1                   |Small                 |Small                |Small                 |Small                |
|          5.0|         3.6|          1.4|         0.2|setosa  |1                    |3                   |1                    |1                   |Small                 |Large                |Small                 |Small                |
|          5.4|         3.9|          1.7|         0.4|setosa  |1                    |3                   |1                    |1                   |Small                 |Large                |Small                 |Small                |1

## Methods

Classification is a fundamental task in data analysis and pattern recognition where the goal is to build a classifierâ€”a function that assigns class labels to instances based on their attributes. Inducing classifiers from datasets of preclassified instances is a key challenge in machine learning. Many methods address this challenge using diverse functional representations like decision trees, decision lists, neural networks, decision graphs, and rules. One of the most effective classifiers, in the sense that its predictive performance is competitive
with state-of-the-art classifiers, is the so-called naive Bayesian classifier.[@friedman1997bayesian]


Define Variables and Dependencies:

Assign Conditional Probability Tables (CPTs): 

Inference:

Learning: structure and parameter 

<p style="color: blue;">Idea - Define the problem.</p>
<p style="color: blue;">Idea - Define the structure.</p>
<p style="color: blue;">Idea - create the paramter learning CPT</p>
<p style="color: blue;">Idea - use belief propagation to comput probabilities</p>

## Analysis and Results
Decision Support:

Validation and Sensitivity Analysis:
monte carlo 

<p style="color: blue;">Idea - Evalulate the networks ability </p>

<p style="color: blue;">Idea - Cross Validate to test models robustness and generalization</p>

<p style="color: blue;">Idea - Asses how sensitive network is to input and structure chantes</p>

<p style="color: blue;">Idea -Use the networks to make a prediction</p>


## Data and Visualization
directed acyclic graph (DAG).

Conditional Probability Tables (CPTs)

Probability Distributions:

## Statistical Modeling
Add discription of final model possibly after learning

### Conclusion
Add conclusion here

## References
